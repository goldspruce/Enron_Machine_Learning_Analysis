{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Logistic Regression\n",
    "\n",
    "Learning Goals:  \n",
    "* Be able to explain how logistic regression arises from linear regression  \n",
    "* Explain the shortcomings of using accuracy as a metric, and use an alternative  \n",
    "* Iterate on an algorithm to improve its results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data and define the problem\n",
    "\n",
    "#### Data Set Information:  \n",
    "\n",
    "This data approach student achievement in secondary education of two Portuguese schools. The data attributes include student grades, demographic, social and school related features) and it was collected by using school reports and questionnaires. Two datasets are provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). In [Cortez and Silva, 2008], the two datasets were modeled under binary/five-level classification and regression tasks. Important note: the target attribute G3 has a strong correlation with attributes G2 and G1. This occurs because G3 is the final year grade (issued at the 3rd period), while G1 and G2 correspond to the 1st and 2nd period grades. It is more difficult to predict G3 without G2 and G1, but such prediction is much more useful (see paper source for more details).  \n",
    "[source, with column descriptions](https://archive.ics.uci.edu/ml/datasets/Student+Performance#)\n",
    "\n",
    "#### The Problem\n",
    "Optimize a classification model to predict which students are likely to pass their math classes. This model should not use columns G2 and G3 as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'school', u'sex', u'age', u'address', u'famsize', u'Pstatus', u'Medu',\n",
       "       u'Fedu', u'Mjob', u'Fjob', u'reason', u'guardian', u'traveltime',\n",
       "       u'studytime', u'failures', u'schoolsup', u'famsup', u'paid',\n",
       "       u'activities', u'nursery', u'higher', u'internet', u'romantic',\n",
       "       u'famrel', u'freetime', u'goout', u'Dalc', u'Walc', u'health',\n",
       "       u'absences', u'G1', u'G2', u'G3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_df = pd.read_csv('student-mat.csv', sep = ';')\n",
    "\n",
    "math_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make this a classification problem\n",
    "\n",
    "Since we are interested in classification algorithms, we need to turn our grade variable (G3) into a categorical value. Not knowing more about Portuguese grading scales, we will assume that a score at or above the 70th percentile is required to pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# caluclate the 70th percentile of the G3 column\n",
    "np.percentile(math_df.G3, 70.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and add a column for pass/fail\n",
    "def pass_fail(student):\n",
    "    if student['G3'] >= 13:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "math_df['passed'] = math_df.apply(pass_fail, axis = 1)\n",
    "\n",
    "math_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put data into arrays, and train/test split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.array(math_df[['Medu', 'Fedu']])\n",
    "labels = np.array(math_df['passed'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.20\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    features, labels, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "First we will perform a basic logistic regression. This will provide the intercept and coefficients for later steps, as well as give a baseline for analysis and iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(features_train, labels_train) \n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "acc = accuracy_score(pred, labels_test)\n",
    "print \"accuracy:\", acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the intercept and coefficients\n",
    "intercept = clf.intercept_\n",
    "coefficients = clf.coef_[0]\n",
    "\n",
    "print intercept\n",
    "print coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a dataframe of testing features and labels to use later for comparison\n",
    "\n",
    "math_test = pd.DataFrame(features_test, columns = ['Medu', 'Fedu'])\n",
    "math_test['passed'] = labels_test\n",
    "math_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a column to dataframe showing output from linear model (y_star)\n",
    "\n",
    "math_train['y_star'] = intercept + coefficients[0] * math_train['Medu'] + \\\n",
    "                    coefficients[1] * math_train['Fedu']\n",
    "    \n",
    "print math_train.head()\n",
    "print max(math_train.y_star), min(math_train.y_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use exponential transformation to caluclate probability\n",
    "def get_prob(row):\n",
    "    y = row['y_star']\n",
    "    p = math.exp(y) / (math.exp(y) + 1)\n",
    "    return p\n",
    "\n",
    "math_train['prob'] = math_train.apply(get_prob, axis = 1)\n",
    "\n",
    "math_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add predictions to dataframe for comparison\n",
    "math_train['pred'] = pred\n",
    "\n",
    "math_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss: What was our accuracy before? Looking at the dataframe we just created, is that meaningful? Why or why not?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "**Precision**  \n",
    "Precision is how good the model is at detecting only true positives (false positives hurt your score).  \n",
    "precision = true positives / (true positives + false positives)  \n",
    "\n",
    "**Recall** . \n",
    "Recall is how good the model is at detecting positives overall (doesn't care about false positives).  \n",
    "recall = true positives / (true positives + false negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(labels_test, pred)\n",
    "recall = recall_score(labels_test, pred)\n",
    "print precision\n",
    "print recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss: Remember that we're trying to predict who will pass a class. In this situation, is it better to have a Type I error (false positive) or Type II error (false negative)? What metric is appropriate to use for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## You Do:\n",
    "Try using different features or changing other parameters to build a better classifier. Use a metric other than accuracy as your guide. Do at least 3 iterations using Logistic Regression before moving on to another algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ipykernel_py2]",
   "language": "python",
   "name": "Python [ipykernel_py2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
